{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add id to original label guided dataset\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "file_a_path = \"../VariErr-Label-Guided-longest.json\"\n",
    "file_b_path = \"../varierr.json\"\n",
    "output_path = \"../VariErr-Label-Guided-longest-with-ID.json\"\n",
    "\n",
    "with open(file_b_path, \"r\") as f:\n",
    "    full_dataset = [json.loads(line) for line in f]\n",
    "    pair_to_id = {\n",
    "        (sample[\"context\"].strip(), sample[\"statement\"].strip()): sample[\"id\"]\n",
    "        for sample in full_dataset\n",
    "    }\n",
    "\n",
    "print(f\"We have {len(pair_to_id)} pairs.\")\n",
    "\n",
    "with open(file_a_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(output_path, \"w\") as f_out:\n",
    "    for sample in tqdm(data):\n",
    "        premise = sample[\"premise\"].strip()\n",
    "        hypothesis = sample[\"hypothesis\"].strip()\n",
    "        key = (premise, hypothesis)\n",
    "\n",
    "        if key in pair_to_id:\n",
    "            sample[\"id\"] = pair_to_id[key]\n",
    "        else:\n",
    "            print(f\"Not found: premise='{premise}...', hypothesis='{hypothesis}...'\")\n",
    "            sample[\"id\"] = None\n",
    "\n",
    "        f_out.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate explanations generated by LLMs to a singel file\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "explanation_root = Path(\"../gpt_4.1_generation_raw\")\n",
    "input_jsonl = Path(\"../varierr.json\")\n",
    "output_jsonl = Path(\"../gpt_4.1_explanation_raw.jsonl\")\n",
    "\n",
    "# suffix = \".txt\"\n",
    "\n",
    "def clean_explanation(text: str) -> str:\n",
    "    return re.sub(r\"^\\s*(?:[\\d]+[\\.\\)]|[-•*]|[a-zA-Z][\\.\\)]|\\(\\w+\\))\\s+\", \"\", text).strip()\n",
    "\n",
    "label_map = {\"E\": \"e\", \"N\": \"n\", \"C\": \"c\"}\n",
    "\n",
    "with open(input_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
    "    instances = [json.loads(line) for line in f]\n",
    "\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for instance in tqdm(instances, desc=\"Inject explanations\"):\n",
    "        sample_id = str(instance[\"id\"])\n",
    "        subfolder = explanation_root / sample_id\n",
    "        new_comments = []\n",
    "\n",
    "        if not subfolder.exists():\n",
    "            print(f\"missing folder: {subfolder}\")\n",
    "        else:\n",
    "            for label in [\"E\", \"N\", \"C\"]:\n",
    "                tried_files = [\n",
    "                    f\"{label}_third.txt\"\n",
    "                    # f\"{label}_second.txt\",\n",
    "                    # f\"{label}_first.txt\",\n",
    "                    # label,\n",
    "                ]\n",
    "        \n",
    "                file_found = False\n",
    "                for fname in tried_files:\n",
    "                    file_path = subfolder / f\"{fname}\"\n",
    "                    if file_path.exists():\n",
    "                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            explanations = [\n",
    "                                clean_explanation(line)\n",
    "                                for line in f\n",
    "                                if line.strip()\n",
    "                            ]\n",
    "                        new_comments.extend([[exp, label_map[label]] for exp in explanations])\n",
    "                        file_found = True\n",
    "                        break\n",
    "\n",
    "                if not file_found:\n",
    "                    print(f\"No file found for {label} in {subfolder}\")\n",
    "        new_instance = {\n",
    "            \"id\": instance[\"id\"],\n",
    "            \"premise\": instance[\"context\"],\n",
    "            \"hypothesis\": instance[\"statement\"],\n",
    "            \"generated_explanations\": new_comments\n",
    "        }\n",
    "\n",
    "        fout.write(json.dumps(new_instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "ROOT_FOLDER = \"../llama_elaborate_generation_raw\"\n",
    "\n",
    "def count_generations(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return sum(1 for line in f if line.strip())\n",
    "\n",
    "def count_all_generations():\n",
    "    records = []\n",
    "    total = {\"E\": 0, \"N\": 0, \"C\": 0}\n",
    "\n",
    "    for subfolder in os.listdir(ROOT_FOLDER):\n",
    "        sub_path = os.path.join(ROOT_FOLDER, subfolder)\n",
    "        if not os.path.isdir(sub_path):\n",
    "            continue\n",
    "\n",
    "        row = {\"folder\": subfolder}\n",
    "        for label in [\"E\", \"N\", \"C\"]:\n",
    "            file_path = os.path.join(sub_path, f\"{label}_third.txt\")\n",
    "            if os.path.isfile(file_path):\n",
    "                count = count_generations(file_path)\n",
    "            else:\n",
    "                count = 0\n",
    "            row[label] = count\n",
    "            total[label] += count\n",
    "\n",
    "        records.append(row)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.loc[\"TOTAL\"] = [\"TOTAL\"] + [total[\"E\"], total[\"N\"], total[\"C\"]]\n",
    "    # print(df)\n",
    "\n",
    "    print(f\"]E={total['E']}，N={total['N']}，C={total['C']}，total: {total['E'] + total['N'] + total['C']}\")\n",
    "\n",
    "count_all_generations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get avg score for instance\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('../scores.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for key, value in data.items():\n",
    "    try:\n",
    "        id_label, _ = key.rsplit('-', 1)\n",
    "        groups[id_label].append(value)\n",
    "    except ValueError:\n",
    "        print(f\"'{key}' does not match.\")\n",
    "        continue\n",
    "\n",
    "averaged_data = {k: sum(v) / len(v) for k, v in groups.items()}\n",
    "with open('../avg_llama3.1_scores.jsonn', 'w') as f:\n",
    "    json.dump(averaged_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thresholding for ChaosNLI\n",
    "import json\n",
    "\n",
    "def process_label_distribution(label_probs, threshold=0.2):\n",
    "    valid_indices = [i for i, p in enumerate(label_probs) if p >= threshold]\n",
    "    count = len(valid_indices)\n",
    "    if count == 0:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    return [1.0 / count if i in valid_indices else 0.0 for i in range(3)]\n",
    "\n",
    "input_file = '../dev_cleaned.json'\n",
    "output_file = '../dev_cleaned_20.json'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as fin, open(output_file, 'w', encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        item = json.loads(line)\n",
    "        raw_label = item['label']\n",
    "        new_label = process_label_distribution(raw_label)\n",
    "        item['label'] = new_label\n",
    "        fout.write(json.dumps(item, ensure_ascii=False) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
